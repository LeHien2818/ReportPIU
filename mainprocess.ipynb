{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302},{"sourceId":9448132,"sourceType":"datasetVersion","datasetId":5742470},{"sourceId":9448136,"sourceType":"datasetVersion","datasetId":5742473},{"sourceId":10229352,"sourceType":"datasetVersion","datasetId":6324642},{"sourceId":10233697,"sourceType":"datasetVersion","datasetId":6327724}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I run a model from last year’s competition to detect sleep periods based on anglez and enmo. \n\nAfter detecting these sleep periods, I generated some features relevant to them.\n\n\nNotes:\n- I ran one of my models from last year. For more details, please visit [here](https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/459597)\n- Approximately 60% of the data has timesteps of 5 seconds, while the remaining data does not. This notebook does not address this noise.\n- Time zone information is not included; I expect that it has already been corrected by host.","metadata":{}},{"cell_type":"code","source":"import datetime\nimport gc\nimport os\nimport sys\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport torch\nimport yaml\nfrom tqdm import tqdm\n\n# TRAIN_OR_TEST = \"train\"\n\n# paths = glob(\n#     f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_{TRAIN_OR_TEST}.parquet/id=*/part-0.parquet\"\n# )\n# print(len(paths))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-18T17:03:00.975633Z","iopub.execute_input":"2024-12-18T17:03:00.976356Z","iopub.status.idle":"2024-12-18T17:03:00.981004Z","shell.execute_reply.started":"2024-12-18T17:03:00.976302Z","shell.execute_reply":"2024-12-18T17:03:00.980107Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import random\nimport torch\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(2024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:00.985018Z","iopub.execute_input":"2024-12-18T17:03:00.985382Z","iopub.status.idle":"2024-12-18T17:03:00.991760Z","shell.execute_reply.started":"2024-12-18T17:03:00.985345Z","shell.execute_reply":"2024-12-18T17:03:00.991014Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"MAX_FILE = 2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:00.993891Z","iopub.execute_input":"2024-12-18T17:03:00.994630Z","iopub.status.idle":"2024-12-18T17:03:00.999862Z","shell.execute_reply.started":"2024-12-18T17:03:00.994583Z","shell.execute_reply":"2024-12-18T17:03:00.999098Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Preprocess\n\nFirst, prepare the features used in my sleep detection model. Please refer to the implementation by [@tatamikenn](https://www.kaggle.com/tatamikenn) [here](https://www.kaggle.com/code/tatamikenn/sleep-hdcza-a-pure-heuristic-approach-lb-0-447).","metadata":{}},{"cell_type":"code","source":"def transform(df, night_offset=20):\n    return (\n        df.with_columns(\n            [\n                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n                pl.col(\"timestamp\").dt.minute().cast(pl.Int8).alias(\"minute\"),\n                pl.col(\"timestamp\").dt.second().cast(pl.Int8).alias(\"second\"),\n                pl.col(\"timestamp\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n            ]\n        )\n        .with_columns( \n            pl.when(pl.col(\"hour\") < night_offset)\n            .then(pl.col(\"timestamp\"))\n            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n            .dt.date()\n            .alias(\"night_group\"),\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"series_id\") + pl.lit(\"_\") + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n                ).alias(\"group_id\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"timestamp\").cum_count().over(\"group_id\").alias(\"norm_step\"),\n            ]\n        )\n        .drop([\"night_group\"])\n    )\n\n\ndef transform_series(df):\n    return transform(df).with_columns(\n        [\n            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n        ]\n    )\n\n\ndef transform_events(df):\n    return (\n        transform(df)\n        .with_columns(\n            [\n                pl.col(\"night\").cast(pl.UInt32).alias(\"night\"),\n            ]\n        )\n        .pivot([\"step\", \"timestamp\", \"tz_offset\"], [\"series_id\", \"group_id\", \"night\"], \"event\")\n    )\n\n\ndef add_feature(\n    df,\n    day_group_col=\"group_id\",\n    term1=(5 * 60) // 5,\n    term2=(30 * 60) // 5,\n    term3=(60 * 60) // 5,\n    min_threshold=0.005,\n    max_threshold=0.04,\n    center=True,\n):\n    return (\n        df.with_columns(\n            [\n                pl.col(\"anglez\").diff(1).abs().alias(\"anglez_diff\"),\n                pl.col(\"enmo\").diff(1).abs().alias(\"enmo_diff\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"anglez_diff_median_5min\"),\n                pl.col(\"enmo_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"enmo_diff_median_5min\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff_median_5min\")\n                .quantile(0.1)\n                .clip(min_threshold, max_threshold)\n                .over(day_group_col)\n                .alias(\"critical_threshold\")\n            ]\n        )\n        .with_columns([(pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15).alias(\"is_static\")])\n        .with_columns(\n            [\n                pl.col(\"is_static\").cast(pl.Int32).rolling_sum(term2, center=center).alias(\"is_static_sum_30min\"),\n            ]\n        )\n        .with_columns([(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term2 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term2 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n        .with_columns([pl.col(\"is_gap\").cast(pl.Int32).rolling_sum(term3, center=center).alias(\"gap_length\")])\n        .with_columns([(pl.col(\"gap_length\") == term3).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term3 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term3 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n        #\n        # extract longest sleep episode\n        #\n        .with_columns(\n            [\n                # extract false->true transition\n                (\n                    (\n                        pl.col(\"is_sleep_episode\")\n                        & pl.col(\"is_sleep_episode\").shift(1, fill_value=pl.lit(False)).not_()\n                    )\n                    .cum_sum()\n                    .over(\"group_id\")\n                ).alias(\"sleep_episode_id\")\n            ]\n        )\n        .with_columns(\n            [pl.col(\"is_sleep_episode\").sum().over([\"group_id\", \"sleep_episode_id\"]).alias(\"sleep_episode_length\")]\n        )\n        .with_columns([pl.col(\"sleep_episode_length\").max().over([\"group_id\"]).alias(\"max_sleep_episode_length\")])\n        .with_columns(\n            [\n                (\n                    pl.col(\"is_sleep_episode\") & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n                ).alias(\"is_longest_sleep_episode\")\n            ]\n        )\n    )\n\n\nuse_columns = [\n    \"series_id\",\n    \"step\",\n    \"is_longest_sleep_episode\",\n    \"is_sleep_block\",\n    \"is_gap\",\n    \"is_large_gap\",\n    \"is_sleep_episode\",\n    \"is_static\",\n]\n\ndef create_heuristic(paths, train_or_test):\n    i = 0\n    for path in tqdm(paths):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        sdf = pl.read_parquet(path)\n    \n        # dummy timestamp\n        sdf = sdf.with_columns((pl.col(\"time_of_day\") == 0).cast(pl.Int32).cum_sum().alias(\"day_offset\"))\n        sdf = sdf.with_columns(\n            (\n                datetime.datetime(2020, 1, 1)\n                + (pl.col(\"day_offset\") * 86400_000_000 + pl.col(\"time_of_day\") / 1000).cast(pl.Duration(\"us\"))\n            ).alias(\"timestamp\")\n        )\n    \n        sdf = sdf.with_columns(pl.lit(path.split(\"/\")[-2]).alias(\"series_id\"))\n        sdf = sdf.sort(\"step\")\n        sdf = transform_series(sdf)\n        sdf = add_feature(sdf)\n        sdf = sdf[use_columns].fill_null(False)\n    \n        sidf = path.split(\"/\")[-2]\n        save_path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sidf}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        sdf.write_parquet(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.001480Z","iopub.execute_input":"2024-12-18T17:03:01.001782Z","iopub.status.idle":"2024-12-18T17:03:01.023688Z","shell.execute_reply.started":"2024-12-18T17:03:01.001757Z","shell.execute_reply":"2024-12-18T17:03:01.022739Z"},"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Sleep Detection","metadata":{}},{"cell_type":"code","source":"if True:\n    sys.path.append(\"/kaggle/input/cmi-2023-src\")\n    from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n    from torch_models.dataset import ZzzPatchDataset\n    from torch_models.models import ZzzConv1dGRUModel, ZzzTransformerGRUModel, ZzzWaveGRUModel\n\n    from utils.feature_contena import Features\n    from utils.lightning_utils import MyLightningDataModule, MyLightningModule\n    from utils.set_seed import seed_base_torch\n    from utils.torch_template import EnsembleModel","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.024748Z","iopub.execute_input":"2024-12-18T17:03:01.025075Z","iopub.status.idle":"2024-12-18T17:03:01.034512Z","shell.execute_reply.started":"2024-12-18T17:03:01.025026Z","shell.execute_reply":"2024-12-18T17:03:01.033721Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def detection(paths=f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\"):\n    MODEL_NAME = \"patch_transformer_gru\"\n    \n    PACKAGE_DIR = Path(\"/kaggle/input/cmi-2023-src\")\n    CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n    BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]\n    \n    CFG[\"output_dir\"] = f\"/kaggle/input/cmi-2023-output/{CFG[MODEL_NAME]['execution']['best_exp_id']}\"\n    \n    seed_base_torch(CFG[\"env\"][\"seed\"])\n    \n    DEVICE = \"cuda\"\n    \n    files = glob(\n        paths\n    )\n    \n    features = Features()\n    features.add_num_features([\"anglez\", \"enmo\"])\n    features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n    features.add_num_features([\"same_count\"])\n    features.add_num_features([\"large_diff_count\"])\n    features.add_num_features([\"same_count_shift_plus\", \"same_count_shift_minus\"])\n    features.add_num_features([\"is_longest_sleep_episode\", \"is_sleep_block\"])\n    \n    # transformer + gru\n    model = ZzzTransformerGRUModel(\n        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        **CFG[MODEL_NAME][\"params\"],\n    )\n    trn_models = [\n        MyLightningModule.load_from_checkpoint(\n            os.path.join(\"/kaggle/input/cmi-2023-output/exp_160\", f\"logs/best_model_fold{fold}.ckpt\"),\n            model=model,\n            map_location=torch.device(DEVICE),\n        ).to(DEVICE)\n        for fold in range(5 if len(files) > 100 else 1)\n    ]\n    \n    models = trn_models\n    model = EnsembleModel(models).to(DEVICE)\n    model.eval()\n    \n    all_oof_dfs = []\n    i = 0\n    for file in tqdm(files):\n        # load file\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pd.read_parquet(file)\n        if len(df) < BLOCK_SIZE:\n            continue\n        time_of_days = df[\"time_of_day\"].values\n    \n        # same_count\n        DAY_STEPS = 12 * 60 * 24\n        n_days = int(len(df) // DAY_STEPS) + 1\n        df[\"same_count\"] = 0\n        for day in range(-n_days, n_days + 1):\n            if day == 0:\n                continue\n            df[\"_anglez_diff\"] = df[\"anglez\"].diff(DAY_STEPS * day)\n            df[\"_anglez_diff\"] = df[\"_anglez_diff\"].fillna(1)\n            df[\"same_count\"] += (df[\"_anglez_diff\"] == 0).astype(int)\n        df[\"same_count\"] = (df[\"same_count\"].clip(0, 5) - 2.5) / 2.5\n    \n        SHIFT_STEPS = 12 * 60 * 6  # 6h\n        df[\"same_count_shift_plus\"] = df[\"same_count\"].shift(SHIFT_STEPS).fillna(1.0).astype(np.float16)\n        df[\"same_count_shift_minus\"] = df[\"same_count\"].shift(-SHIFT_STEPS).fillna(1.0).astype(np.float16)\n    \n        # features\n        df[\"anglez_diffabs\"] = df[\"anglez\"].diff().abs().fillna(0)\n        df[\"large_diff\"] = (df[\"anglez_diffabs\"] > 5).astype(int)\n        df[\"large_diff_count\"] = df[\"large_diff\"].rolling(10, center=True).mean().fillna(0)\n        df[\"large_diff_count\"] = (df[\"large_diff_count\"] - 0.5) * 2\n    \n        # normalize\n        df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n        df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n        df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n        df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n    \n        # heuristic_features by @bilzard\n        sid = file.split(\"/\")[-2]\n        df[\"series_id\"] = sid\n        path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sid}.parquet\"\n        hdf = pd.read_parquet(path)\n        df = pd.concat([df, hdf.drop(columns=[\"series_id\", \"step\"])], axis=1)\n        df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] = df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] * 2 - 1\n    \n        # split\n        dfs = []\n        df = df.sort_values(\"step\").reset_index(drop=True)\n        for start in range(0, len(df), BLOCK_SIZE // 8):\n            end = start + BLOCK_SIZE\n            if end > len(df):\n                end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n                start = end - BLOCK_SIZE\n                assert start >= 0\n            assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n            dfs.append(df.iloc[start:end])\n        gc.collect()\n    \n        # inference\n        train_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        valid_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n        preds = []\n        with torch.no_grad():\n            for X in data_module.val_dataloader():\n                pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n                preds.append(pred)\n    \n        oof_dfs = []\n        for pred, df in zip(np.vstack(preds), dfs):\n            df = df.iloc[\n                CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 : len(df) : CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n            ].reset_index(drop=True)\n            df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n            oof_dfs.append(df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n    \n        oof_df = pd.concat(oof_dfs)\n        oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n        oof_df = oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]]\n        oof_df[\"step\"] = oof_df[\"step\"].astype(int)\n    \n        del preds, oof_dfs\n        gc.collect()\n    \n        train = oof_df.reset_index(drop=True)\n        train[\"time_of_day\"] = time_of_days[\n            CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 :: CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        ][: len(train)]\n        all_oof_dfs.append(train[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\", \"time_of_day\"]])\n        # del dfs, df\n        gc.collect()\n\n    # save\n    for df in tqdm(all_oof_dfs):\n        save_path = f\"/kaggle/working/features/sleep_detection/{train_or_test}/{df['series_id'].iloc[0]}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        df.to_parquet(save_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.157447Z","iopub.execute_input":"2024-12-18T17:03:01.157671Z","iopub.status.idle":"2024-12-18T17:03:01.178881Z","shell.execute_reply.started":"2024-12-18T17:03:01.157648Z","shell.execute_reply":"2024-12-18T17:03:01.177969Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # check!\n# sample_file = \"/kaggle/working/features/sleep_detection/id=0d279d77.parquet\"\n\n# df = pl.read_parquet(sample_file)\n# df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n# sid = df[\"series_id\"][0]\n\n# sensor_df = pl.read_parquet(\n#     f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_{TRAIN_OR_TEST}.parquet/{sid}/part-0.parquet\"\n# ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n\n# sensor_df = sensor_df.join(df, on=\"step\", how=\"left\").with_columns(\n#     pl.col(\"onset_oof\").interpolate(),\n#     pl.col(\"wakeup_oof\").interpolate(),\n# )\n\n# for (day, ), day_df in sensor_df.group_by(\"day\", maintain_order=True):\n#     fig, axs = plt.subplots(3, 1, figsize=(20, 3))\n#     times = np.linspace(0, 24, len(day_df))\n#     axs[0].plot(times, day_df[\"enmo\"])\n#     axs[0].set_ylabel(\"enmo\")\n#     axs[1].plot(times, day_df[\"anglez\"])\n#     axs[1].set_ylabel(\"anglez\")\n#     axs[2].plot(times, day_df[\"onset_oof\"])\n#     axs[2].plot(times, day_df[\"wakeup_oof\"])\n#     axs[2].set_ylabel(\"oof\")\n#     axs[2].set_ylim(0, 10)\n#     plt.tight_layout()\n#     plt.show()\n#     if day > 5:\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.180304Z","iopub.execute_input":"2024-12-18T17:03:01.180581Z","iopub.status.idle":"2024-12-18T17:03:01.192008Z","shell.execute_reply.started":"2024-12-18T17:03:01.180557Z","shell.execute_reply":"2024-12-18T17:03:01.191167Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"time_of_day_max = 86400000000000\n# all_files = sorted(glob(\"/kaggle/working/features/sleep_detection/*.parquet\"))\n# len(all_files)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.192924Z","iopub.execute_input":"2024-12-18T17:03:01.193172Z","iopub.status.idle":"2024-12-18T17:03:01.202894Z","shell.execute_reply.started":"2024-12-18T17:03:01.193149Z","shell.execute_reply":"2024-12-18T17:03:01.202092Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def feature_engineering(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\"):\n    features = []\n    debug_count = 0\n    all_files = sorted(glob(paths))\n    i = 0\n    for file in tqdm(all_files):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pl.read_parquet(file)\n        df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n        sid = df[\"series_id\"][0]\n    \n        sensor_df = pl.read_parquet(\n            f\"{data_paths}/{sid}/part-0.parquet\"\n        ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n    \n        feature = {\n            \"id\": sid,\n            \"length\": df.shape[0],\n            \"day\": sensor_df[\"relative_date_PCIAT\"].max() - sensor_df[\"relative_date_PCIAT\"].min(),\n        }\n    \n        # skip if time step is not 5sec\n        diffs = sensor_df[\"time_of_day\"].diff().drop_nulls().unique()\n        if set(diffs) != set([-86395000000000, 5000000000]):\n            features.append(feature)\n            continue\n    \n        sensor_df = (\n            sensor_df.join(df, on=\"step\", how=\"left\")\n            .sort(\"step\")\n            .with_columns(\n                pl.col(\"onset_oof\").interpolate(),\n                pl.col(\"wakeup_oof\").interpolate(),\n            )\n        )\n    \n        # onset = 15:00~3:00, wakeup = 3:00~15:00\n        onset_start = time_of_day_max / 24 * 15  # 15:00\n        onset_end = time_of_day_max / 24 * 3  # 3:00\n        sensor_df = sensor_df.with_columns(\n            ((pl.col(\"time_of_day\") > onset_start) | (pl.col(\"time_of_day\") < onset_end)).alias(\"onset_duration\"),\n        ).with_columns(\n            pl.col(\"onset_duration\").cast(pl.Int32).diff().fill_null(0).abs().cum_sum().alias(\"onset_wakeup_duration\")\n        )\n    \n        # get sleep period\n        sleep_info = []\n        for _, df in sensor_df.group_by(\"onset_wakeup_duration\", maintain_order=True):\n            is_onset = df[\"onset_duration\"][0]\n            if is_onset:\n                max_idx = df[\"onset_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"onset_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                if start_time >= 15:\n                    day = df[\"day\"][0]\n                    week_day = df[\"weekday\"][0]\n                else:\n                    day = df[\"day\"][0] - 1\n                    week_day = df[\"weekday\"][0] - 1\n                    if week_day == 0:\n                        week_day = 7\n            else:\n                max_idx = df[\"wakeup_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"wakeup_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                day = df[\"day\"][0] - 1\n                week_day = df[\"weekday\"][0] - 1\n    \n            info = {\n                \"day\": day,\n                \"weekday\": week_day,\n                \"type\": \"onset\" if is_onset else \"wakeup\",\n                \"step\": step,\n                \"max_score\": max_score,\n                \"time\": df[\"time_of_day\"][max_idx] / time_of_day_max * 24,\n            }\n            sleep_info.append(info)\n        sleep_df = pl.DataFrame(sleep_info)\n    \n        # merge\n        sleep_df = (\n            sleep_df.filter(pl.col(\"type\") == \"onset\")\n            .drop(\"type\")\n            .rename(\n                {\n                    \"max_score\": \"onset_score\",\n                    \"step\": \"onset_step\",\n                    \"time\": \"onset_time\",\n                }\n            )\n            .join(\n                sleep_df.filter(pl.col(\"type\") == \"wakeup\")\n                .drop([\"type\", \"weekday\"])\n                .rename(\n                    {\n                        \"max_score\": \"wakeup_score\",\n                        \"step\": \"wakeup_step\",\n                        \"time\": \"wakeup_time\",\n                    }\n                ),\n                on=\"day\",\n            )\n        ).select(\n            [\"day\", \"weekday\", \"onset_time\", \"wakeup_time\", \"onset_step\", \"wakeup_step\", \"onset_score\", \"wakeup_score\"]\n        )\n    \n        # feature engineering\n        sleep_lengths = []  # wakeup - onset\n        sleep_enmo_mean = []  \n        sleep_enmo_std = []  \n        sleep_light_mean = []\n        sleep_light_std = [] \n        for i in range(len(sleep_df)):\n            # sleep period\n            start = sleep_df[\"onset_step\"][i]\n            end = sleep_df[\"wakeup_step\"][i]\n            if sleep_df[\"onset_score\"][i] < 1 or sleep_df[\"wakeup_score\"][i] < 1:\n                sleep_lengths.append(np.nan)\n                sleep_enmo_mean.append(np.nan)\n                sleep_enmo_std.append(np.nan)\n                sleep_light_mean.append(np.nan)\n                sleep_light_std.append(np.nan)\n                continue\n    \n            # sleep length\n            length = end - start\n            sleep_lengths.append(length * 5 / 60 / 60)  # hour\n    \n            # enmo\n            enmo_mean = sensor_df[\"enmo\"][start:end].mean()\n            enmo_std = sensor_df[\"enmo\"][start:end].std()\n            sleep_enmo_mean.append(enmo_mean)\n            sleep_enmo_std.append(enmo_std)\n    \n            # light\n            light_mean = sensor_df[\"light\"][start:end].mean()\n            light_std = sensor_df[\"light\"][start:end].std()\n            sleep_light_mean.append(light_mean)\n            sleep_light_std.append(light_std)\n            \n        sleep_df = sleep_df.with_columns(\n            pl.DataFrame(\n                {\n                    \"sleep_length\": sleep_lengths,\n                    \"sleep_enmo_mean\": sleep_enmo_mean,\n                    \"sleep_enmo_std\": sleep_enmo_std,\n                    \"sleep_light_mean\": sleep_light_mean,\n                    \"sleep_light_std\": sleep_light_std,\n                }\n            )\n        )\n        \n        # leave only high confidence periods\n        sleep_df = sleep_df.filter((pl.col(\"wakeup_score\") > 1) & (pl.col(\"onset_score\") > 1))\n        if debug_count < 3:\n            display(sleep_df.head())\n        debug_count += 1\n            \n    \n        # agg\n        feature.update(\n            {\n                \"sleep_measurement_count\": sleep_df.shape[0],\n                \"sleep_length_mean\": sleep_df[\"sleep_length\"].mean(),\n                \"sleep_length_std\": sleep_df[\"sleep_length\"].std(),\n                \"sleep_start_mean\": sleep_df[\"onset_time\"].mean(),\n                \"sleep_start_std\": sleep_df[\"onset_time\"].std(),\n                \"sleep_end_mean\": sleep_df[\"wakeup_time\"].mean(),\n                \"sleep_end_std\": sleep_df[\"wakeup_time\"].std(),\n                \"sleep_enmo_mean_mean\": sleep_df[\"sleep_enmo_mean\"].mean(),\n                \"sleep_enmo_mean_std\": sleep_df[\"sleep_enmo_mean\"].std(),\n                \"sleep_enmo_std_mean\": sleep_df[\"sleep_enmo_std\"].mean(),\n                \"sleep_enmo_std_std\": sleep_df[\"sleep_enmo_std\"].std(),\n                \"sleep_light_mean_mean\": sleep_df[\"sleep_light_mean\"].mean(),\n                \"sleep_light_mean_std\": sleep_df[\"sleep_light_mean\"].std(),\n                \"sleep_light_std_mean\": sleep_df[\"sleep_light_std\"].mean(),\n                \"sleep_light_std_std\": sleep_df[\"sleep_light_std\"].std(),\n            }\n        )\n        features.append(feature)\n    output_dir = f\"/kaggle/working/features/{train_or_test}\"\n    os.makedirs(output_dir, exist_ok=True)\n    feature_df = pl.DataFrame(features).with_columns(pl.col(\"id\").str.slice(3, 8))\n    feature_df.write_csv(f\"/kaggle/working/features/{train_or_test}/sleep_features.csv\")\n    print(feature_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:01.204539Z","iopub.execute_input":"2024-12-18T17:03:01.204803Z","iopub.status.idle":"2024-12-18T17:03:01.225409Z","shell.execute_reply.started":"2024-12-18T17:03:01.204780Z","shell.execute_reply":"2024-12-18T17:03:01.224633Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\"), train_or_test=\"test\")\ndetection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\", train_or_test=\"test\")\nfeature_engineering(paths=\"/kaggle/working/features/sleep_detection/test/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\", train_or_test=\"test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:01.226350Z","iopub.execute_input":"2024-12-18T17:03:01.226605Z","iopub.status.idle":"2024-12-18T17:03:06.438316Z","shell.execute_reply.started":"2024-12-18T17:03:01.226581Z","shell.execute_reply":"2024-12-18T17:03:06.437389Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00,  4.37it/s]\n100%|██████████| 2/2 [00:04<00:00,  2.22s/it]\n100%|██████████| 2/2 [00:00<00:00, 78.35it/s]\n  0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"shape: (5, 13)\n┌─────┬─────────┬────────────┬─────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n│ day ┆ weekday ┆ onset_time ┆ wakeup_time ┆ … ┆ sleep_enmo ┆ sleep_enmo ┆ sleep_ligh ┆ sleep_ligh │\n│ --- ┆ ---     ┆ ---        ┆ ---         ┆   ┆ _mean      ┆ _std       ┆ t_mean     ┆ t_std      │\n│ i64 ┆ i64     ┆ f64        ┆ f64         ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n│     ┆         ┆            ┆             ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n╞═════╪═════════╪════════════╪═════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n│ 0   ┆ 2       ┆ 22.091667  ┆ 7.041667    ┆ … ┆ 0.003588   ┆ 0.008487   ┆ 2.06153    ┆ 0.52031    │\n│ 1   ┆ 3       ┆ 22.641667  ┆ 8.141667    ┆ … ┆ 0.002427   ┆ 0.006132   ┆ 2.714605   ┆ 1.259364   │\n│ 2   ┆ 4       ┆ 21.575     ┆ 7.558333    ┆ … ┆ 0.003959   ┆ 0.007149   ┆ 6.441472   ┆ 2.73119    │\n│ 3   ┆ 5       ┆ 23.141667  ┆ 8.241667    ┆ … ┆ 0.006016   ┆ 0.007928   ┆ 9.246452   ┆ 14.259801  │\n│ 4   ┆ 6       ┆ 22.925     ┆ 7.008333    ┆ … ┆ 0.009862   ┆ 0.012524   ┆ 0.511077   ┆ 0.28813    │\n└─────┴─────────┴────────────┴─────────────┴───┴────────────┴────────────┴────────────┴────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>day</th><th>weekday</th><th>onset_time</th><th>wakeup_time</th><th>onset_step</th><th>wakeup_step</th><th>onset_score</th><th>wakeup_score</th><th>sleep_length</th><th>sleep_enmo_mean</th><th>sleep_enmo_std</th><th>sleep_light_mean</th><th>sleep_light_std</th></tr><tr><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2</td><td>22.091667</td><td>7.041667</td><td>7854</td><td>14298</td><td>5.939998</td><td>6.883044</td><td>8.95</td><td>0.003588</td><td>0.008487</td><td>2.06153</td><td>0.52031</td></tr><tr><td>1</td><td>3</td><td>22.641667</td><td>8.141667</td><td>25530</td><td>32370</td><td>7.395478</td><td>2.857819</td><td>9.5</td><td>0.002427</td><td>0.006132</td><td>2.714605</td><td>1.259364</td></tr><tr><td>2</td><td>4</td><td>21.575</td><td>7.558333</td><td>42042</td><td>49230</td><td>5.715631</td><td>6.638568</td><td>9.983333</td><td>0.003959</td><td>0.007149</td><td>6.441472</td><td>2.73119</td></tr><tr><td>3</td><td>5</td><td>23.141667</td><td>8.241667</td><td>60450</td><td>67002</td><td>8.010484</td><td>3.987538</td><td>9.1</td><td>0.006016</td><td>0.007928</td><td>9.246452</td><td>14.259801</td></tr><tr><td>4</td><td>6</td><td>22.925</td><td>7.008333</td><td>77574</td><td>83394</td><td>8.050978</td><td>2.848315</td><td>8.083333</td><td>0.009862</td><td>0.012524</td><td>0.511077</td><td>0.28813</td></tr></tbody></table></div>"},"metadata":{}},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 25.85it/s]","output_type":"stream"},{"name":"stdout","text":"shape: (2, 18)\n┌──────────┬────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n│ id       ┆ length ┆ day  ┆ sleep_measu ┆ … ┆ sleep_light ┆ sleep_light ┆ sleep_ligh ┆ sleep_ligh │\n│ ---      ┆ ---    ┆ ---  ┆ rement_coun ┆   ┆ _mean_mean  ┆ _mean_std   ┆ t_std_mean ┆ t_std_std  │\n│ str      ┆ i64    ┆ f64  ┆ t           ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n│          ┆        ┆      ┆ ---         ┆   ┆ f64         ┆ f64         ┆ f64        ┆ f64        │\n│          ┆        ┆      ┆ i64         ┆   ┆             ┆             ┆            ┆            │\n╞══════════╪════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n│ 00115b9f ┆ 3610   ┆ 44.0 ┆ null        ┆ … ┆ null        ┆ null        ┆ null       ┆ null       │\n│ 001f3379 ┆ 33033  ┆ 23.0 ┆ 7           ┆ … ┆ 4.917133    ┆ 4.370878    ┆ 3.281733   ┆ 4.990464   │\n└──────────┴────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\"), train_or_test=\"train\")\n# detection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\")\n# feature_engineering(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T17:03:06.439382Z","iopub.execute_input":"2024-12-18T17:03:06.439655Z","iopub.status.idle":"2024-12-18T17:03:06.443717Z","shell.execute_reply.started":"2024-12-18T17:03:06.439630Z","shell.execute_reply":"2024-12-18T17:03:06.442594Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport copy\nimport pickle\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\n\nimport plotly.express as px\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\nSEED = 42\nn_splits = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:06.444704Z","iopub.execute_input":"2024-12-18T17:03:06.444938Z","iopub.status.idle":"2024-12-18T17:03:09.802766Z","shell.execute_reply.started":"2024-12-18T17:03:06.444913Z","shell.execute_reply":"2024-12-18T17:03:09.802059Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:09.803951Z","iopub.execute_input":"2024-12-18T17:03:09.804950Z","iopub.status.idle":"2024-12-18T17:03:09.813705Z","shell.execute_reply.started":"2024-12-18T17:03:09.804919Z","shell.execute_reply":"2024-12-18T17:03:09.812839Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, X, y, test_data):\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n    optimized_thresholds = KappaOPtimizer.x\n    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:09.816073Z","iopub.execute_input":"2024-12-18T17:03:09.816378Z","iopub.status.idle":"2024-12-18T17:03:09.830521Z","shell.execute_reply.started":"2024-12-18T17:03:09.816319Z","shell.execute_reply":"2024-12-18T17:03:09.829778Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntotal_features = list(test.columns)\ntotal_features.remove('id')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:09.831484Z","iopub.execute_input":"2024-12-18T17:03:09.831698Z","iopub.status.idle":"2024-12-18T17:03:09.885644Z","shell.execute_reply.started":"2024-12-18T17:03:09.831676Z","shell.execute_reply":"2024-12-18T17:03:09.885030Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:09.886601Z","iopub.execute_input":"2024-12-18T17:03:09.886916Z","iopub.status.idle":"2024-12-18T17:03:09.892811Z","shell.execute_reply.started":"2024-12-18T17:03:09.886880Z","shell.execute_reply":"2024-12-18T17:03:09.891954Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:03:09.893841Z","iopub.execute_input":"2024-12-18T17:03:09.894126Z","iopub.status.idle":"2024-12-18T17:04:20.282491Z","shell.execute_reply.started":"2024-12-18T17:03:09.894100Z","shell.execute_reply":"2024-12-18T17:04:20.281411Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [01:10<00:00, 14.19it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.64it/s]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"df_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:20.283704Z","iopub.execute_input":"2024-12-18T17:04:20.284019Z","iopub.status.idle":"2024-12-18T17:04:20.289388Z","shell.execute_reply.started":"2024-12-18T17:04:20.283990Z","shell.execute_reply":"2024-12-18T17:04:20.288522Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"df_total = pd.concat([df_train, df_test], axis=0, ignore_index=True)\ndf_total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:20.290298Z","iopub.execute_input":"2024-12-18T17:04:20.290584Z","iopub.status.idle":"2024-12-18T17:04:20.379876Z","shell.execute_reply.started":"2024-12-18T17:04:20.290558Z","shell.execute_reply":"2024-12-18T17:04:20.379164Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"       stat_0    stat_1    stat_2    stat_3    stat_4    stat_5    stat_6  \\\n0     50458.0   50458.0   50458.0   50458.0   50458.0   50458.0   50458.0   \n1    340584.0  340584.0  340584.0  340584.0  340584.0  340584.0  340584.0   \n2     40003.0   40003.0   40003.0   40003.0   40003.0   40003.0   40003.0   \n3    223915.0  223915.0  223915.0  223915.0  223915.0  223915.0  223915.0   \n4     15420.0   15420.0   15420.0   15420.0   15420.0   15420.0   15420.0   \n..        ...       ...       ...       ...       ...       ...       ...   \n993  393240.0  393240.0  393240.0  393240.0  393240.0  393240.0  393240.0   \n994   40085.0   40085.0   40085.0   40085.0   40085.0   40085.0   40085.0   \n995  342324.0  342324.0  342324.0  342324.0  342324.0  342324.0  342324.0   \n996   43330.0   43330.0   43330.0   43330.0   43330.0   43330.0   43330.0   \n997  396396.0  396396.0  396396.0  396396.0  396396.0  396396.0  396396.0   \n\n       stat_7    stat_8    stat_9   stat_10   stat_11   stat_12   stat_13  \\\n0     50458.0   50458.0   50458.0   50458.0   50458.0 -0.054638 -0.163923   \n1    340584.0  340584.0  340584.0  340584.0  340584.0  0.113277  0.093139   \n2     40003.0   40003.0   40003.0   40003.0   40003.0 -0.499738  0.046381   \n3    223915.0  223915.0  223915.0  223915.0  223915.0  0.007430  0.007583   \n4     15420.0   15420.0   15420.0   15420.0   15420.0  0.086653 -0.115162   \n..        ...       ...       ...       ...       ...       ...       ...   \n993  393240.0  393240.0  393240.0  393240.0  393240.0 -0.147508 -0.047232   \n994   40085.0   40085.0   40085.0   40085.0   40085.0 -0.441574 -0.080691   \n995  342324.0  342324.0  342324.0  342324.0  342324.0 -0.181627 -0.300301   \n996   43330.0   43330.0   43330.0   43330.0   43330.0 -0.316384  0.016009   \n997  396396.0  396396.0  396396.0  396396.0  396396.0 -0.004272  0.016859   \n\n      stat_14   stat_15    stat_16   stat_17    stat_18      stat_19  \\\n0   -0.114302  0.045252  -7.805897  0.000000  46.009533  4027.514893   \n1   -0.106038  0.028960  -6.065619  0.046508  56.437958  3829.466064   \n2   -0.181152  0.056544 -11.934993  0.000000  77.305130  4106.425781   \n3   -0.196510  0.053544 -12.847143  0.000000   9.369678  3958.604492   \n4   -0.138969  0.040399 -11.009835  0.000000   5.049157  3992.347656   \n..        ...       ...        ...       ...        ...          ...   \n993 -0.242875  0.027135 -18.903458  0.222337  10.387013  3841.772705   \n994 -0.270330  0.037183 -17.535593  0.000000  11.325677  4123.798828   \n995  0.240738  0.002993  14.728157  0.749290   5.465665  3891.058105   \n996 -0.167890  0.047388 -10.580416  0.000000  42.296310  4053.579102   \n997 -0.631731  0.011926 -55.630768  0.655708  16.771982  3838.189453   \n\n          stat_20   stat_21   stat_22     stat_23   stat_24   stat_25  \\\n0    5.415475e+13  4.438860  2.000000   30.202068  0.633126  0.513286   \n1    4.331149e+13  3.840885  2.000000  232.909103  0.507897  0.541129   \n2    4.481677e+13  3.148264  3.000000  100.144516  0.454021  0.510668   \n3    4.836642e+13  4.273992  2.303057   60.025017  0.586100  0.542189   \n4    5.833895e+13  4.541829  4.000000   46.192024  0.509845  0.494897   \n..            ...       ...       ...         ...       ...       ...   \n993  4.316802e+13  4.002807  1.000000   67.532288  0.478085  0.499994   \n994  4.597792e+13  2.487963  1.000000  154.201294  0.502446  0.457471   \n995  4.310022e+13  4.144179  2.860763   10.118834  0.260311  0.324098   \n996  5.046215e+13  4.470182  3.000000   53.201683  0.453665  0.502702   \n997  4.321212e+13  3.909848  3.000000   79.435593  0.351545  0.303812   \n\n      stat_26   stat_27    stat_28   stat_29     stat_30     stat_31  \\\n0    0.500372  0.132576  34.917873  0.000000  205.862213  108.451317   \n1    0.603787  0.096825  44.034721  0.208482  206.625092  167.600983   \n2    0.412588  0.140594  27.367514  0.000000  274.848145   50.734318   \n3    0.474437  0.103401  32.552841  0.000000   54.104408  122.706802   \n4    0.639449  0.090201  47.933723  0.000000   15.590773  126.121590   \n..        ...       ...        ...       ...         ...         ...   \n993  0.622155  0.109624  48.017563  0.410910   75.709877  164.142853   \n994  0.470241  0.064660  32.590225  0.000000   35.017689   28.219002   \n995  0.799344  0.009499  60.556572  0.421324   29.646894  124.940826   \n996  0.585710  0.106351  42.947170  0.000000  208.168976  112.404045   \n997  0.623476  0.024331  50.303635  0.468723   95.327438  155.573868   \n\n          stat_32   stat_33   stat_34    stat_35   stat_36   stat_37  \\\n0    1.876976e+13  1.825557  0.000000  11.773107 -1.812031 -2.631380   \n1    2.509136e+13  1.957999  0.000000   5.701968 -1.807955 -2.887664   \n2    2.038156e+13  1.169176  0.000000   5.653936 -1.903281 -3.150104   \n3    1.868773e+13  2.023705  1.487018   7.396456 -1.684624 -2.405738   \n4    2.146206e+13  2.081796  0.000000  18.615358 -1.675859 -1.071042   \n..            ...       ...       ...        ...       ...       ...   \n993  2.506494e+13  1.929882  0.000000   6.580971 -1.508058 -2.958281   \n994  2.436134e+13  2.188225  0.000000   0.726917 -1.073320 -1.455156   \n995  2.503642e+13  1.964386  1.455972   5.731455 -1.019361 -1.177506   \n996  1.942842e+13  1.931421  0.000000  14.244914 -1.746094 -2.905339   \n997  2.497264e+13  1.946892  0.000000   6.634319 -1.038711 -1.522690   \n\n      stat_38  stat_39    stat_40  stat_41  stat_42      stat_43  \\\n0   -1.798073      0.0 -89.987045      0.0      0.0  3829.000000   \n1   -1.004992      0.0 -89.654587      0.0      0.0  3098.166748   \n2   -1.020313      0.0 -89.540176      0.0      0.0  3853.000000   \n3   -1.023798      0.0 -89.968369      0.0      0.0  3468.000000   \n4   -1.012266      0.0 -89.770241      0.0      0.0  3815.083252   \n..        ...      ...        ...      ...      ...          ...   \n993 -1.013423      0.0 -89.887924      0.0      0.0  3098.166748   \n994 -1.016536      0.0 -87.998444      0.0      0.0  4073.000000   \n995 -1.011560      0.0 -89.530304      0.0      0.0  3718.000000   \n996 -1.048372      0.0 -89.833092      0.0      0.0  3824.000000   \n997 -1.018787      0.0 -88.761833      0.0      0.0  3098.166748   \n\n          stat_44  stat_45  stat_46  stat_47   stat_48   stat_49   stat_50  \\\n0    0.000000e+00      1.0      2.0     15.0 -0.701660 -0.619076 -0.536432   \n1    0.000000e+00      1.0      2.0    223.0 -0.231743 -0.257600 -0.595426   \n2    4.500000e+10      1.0      3.0     97.0 -0.873151 -0.255299 -0.485521   \n3    0.000000e+00      1.0      1.0     48.0 -0.530198 -0.412805 -0.556091   \n4    3.500000e+10      1.0      4.0     20.0 -0.224805 -0.444297 -0.685736   \n..            ...      ...      ...      ...       ...       ...       ...   \n993  0.000000e+00      1.0      1.0     56.0 -0.552659 -0.354082 -0.850300   \n994  0.000000e+00      1.0      1.0    153.0 -0.831641 -0.369779 -0.664401   \n995  0.000000e+00      1.0      1.0      0.0 -0.267668 -0.609891 -0.947463   \n996  5.500000e+10      1.0      3.0     41.0 -0.684180 -0.309863 -0.649974   \n997  0.000000e+00      1.0      3.0     68.0 -0.052803 -0.044517 -1.009344   \n\n      stat_51    stat_52   stat_53   stat_54      stat_55       stat_56  \\\n0    0.007953 -32.948602  0.000000  2.520257  3958.000000  4.325125e+13   \n1    0.000367 -37.326844  0.000000  4.000000  3724.000000  2.128500e+13   \n2    0.005643 -30.154542  0.000000  2.918126  4089.625000  2.888500e+13   \n3    0.009947 -34.965618  0.000000  0.893617  3841.000000  3.526000e+13   \n4    0.005364 -46.348264  0.000000  1.438378  3837.333252  5.161375e+13   \n..        ...        ...       ...       ...          ...           ...   \n993  0.000000 -58.557291  0.000000  0.555556  3741.000000  2.137000e+13   \n994  0.009702 -41.512409  0.000000  2.748235  4099.000000  2.505000e+13   \n995  0.000000 -72.318169  0.488889  0.872747  3788.000000  2.139500e+13   \n996  0.006432 -41.541863  0.000000  2.392969  4028.666748  3.689000e+13   \n997  0.008622 -88.386049  0.000000  0.500000  3747.000000  2.154000e+13   \n\n     stat_57  stat_58  stat_59   stat_60   stat_61   stat_62   stat_63  \\\n0        3.0      2.0     17.0  0.015846 -0.141810 -0.104193  0.019257   \n1        2.0      2.0    228.0  0.094074  0.068143 -0.228500  0.005257   \n2        3.0      3.0     98.0 -0.644505  0.088542 -0.191693  0.018467   \n3        3.0      1.0     53.0  0.022344  0.009674 -0.245181  0.027653   \n4        3.0      4.0     32.0  0.053034 -0.087422 -0.225430  0.024135   \n..       ...      ...      ...       ...       ...       ...       ...   \n993      2.0      1.0     62.0 -0.112749  0.003331 -0.333463  0.002575   \n994      1.0      1.0    154.0 -0.599089 -0.068216 -0.282813  0.020775   \n995      3.0      1.0      5.0 -0.144425 -0.288146  0.719540  0.000011   \n996      3.0      3.0     42.0 -0.366849  0.024974 -0.245378  0.023637   \n997      2.0      3.0     74.0 -0.020622 -0.028179 -1.007728  0.009831   \n\n       stat_64  stat_65    stat_66  stat_67       stat_68  stat_69  stat_70  \\\n0    -6.358004      0.0   8.230733   4029.0  5.630500e+13      5.0      2.0   \n1   -13.454103      0.0  10.050480   3812.0  4.360500e+13      4.0      2.0   \n2   -11.570901      0.0   7.863636   4111.0  4.727000e+13      3.0      3.0   \n3   -15.000056      0.0   2.340206   3947.0  4.881000e+13      4.0      1.0   \n4   -13.665493      0.0   2.897436   4000.0  6.427000e+13      4.0      4.0   \n..         ...      ...        ...      ...           ...      ...      ...   \n993 -20.125556      0.0   2.107143   3812.0  4.307000e+13      4.0      1.0   \n994 -16.773024      0.0   5.729136   4123.0  4.992000e+13      2.0      1.0   \n995  46.380806      1.0   2.205128   3848.5  4.279000e+13      4.0      4.0   \n996 -15.086617      0.0   6.926828   4070.0  5.347750e+13      5.0      3.0   \n997 -86.119919      1.0   0.879005   3812.0  4.331000e+13      4.0      3.0   \n\n     stat_71   stat_72   stat_73   stat_74   stat_75    stat_76  stat_77  \\\n0       28.0  0.437897  0.148919  0.223770  0.036048  13.095750      0.0   \n1      233.0  0.517859  0.542323  0.312333  0.020598  18.462269      0.0   \n2       99.0 -0.242422  0.381953  0.088555  0.048282   5.009753      0.0   \n3       60.0  0.536801  0.443383  0.084469  0.057278   4.816339      0.0   \n4       42.0  0.544297  0.153125  0.347474  0.043690  20.726226      0.0   \n..       ...       ...       ...       ...       ...        ...      ...   \n993     68.0  0.140716  0.280936  0.231454  0.012770  13.528161      0.0   \n994    154.0 -0.214362  0.210247  0.034375  0.039810   1.885406      0.0   \n995     10.0 -0.061189 -0.132583  0.863528  0.004280  57.963976      1.0   \n996     50.0 -0.010677  0.400677  0.204727  0.041420  12.220764      0.0   \n997     79.0 -0.019081  0.020307 -0.294459  0.010668 -17.483364      1.0   \n\n       stat_78      stat_79       stat_80  stat_81  stat_82  stat_83  \\\n0    24.750000  4146.000000  6.978000e+13      6.0      2.0     38.0   \n1    27.490936  3958.000000  6.511000e+13      5.0      2.0    238.0   \n2    21.022933  4140.000000  6.094500e+13      4.0      3.0    100.0   \n3     6.200000  4064.000000  6.330000e+13      6.0      4.0     67.0   \n4     4.942201  4087.000000  7.393625e+13      7.0      4.0     69.0   \n..         ...          ...           ...      ...      ...      ...   \n993   5.281850  3958.000000  6.502500e+13      6.0      1.0     73.0   \n994  10.699164  4146.000000  6.662500e+13      2.0      1.0    155.0   \n995   5.808605  3982.000000  6.500000e+13      6.0      4.0     15.0   \n996  15.000000  4147.000000  6.640875e+13      6.0      3.0     53.0   \n997   6.141348  3951.187561  6.485500e+13      6.0      3.0     85.0   \n\n      stat_84   stat_85   stat_86   stat_87    stat_88  stat_89      stat_90  \\\n0    1.850391  3.580182  1.738203  5.314874  89.422226      0.0  2626.199951   \n1    1.928769  3.234613  2.475326  3.966906  89.080330      1.0  2628.199951   \n2    1.021510  1.016589  1.746797  5.066334  86.987267      0.0  2618.199951   \n3    5.908000  2.083693  1.269051  6.134459  89.976074      0.0  2502.000000   \n4    3.231563  1.033620  1.071875  2.774382  89.300034      0.0  1046.800049   \n..        ...       ...       ...       ...        ...      ...          ...   \n993  0.999923  1.043029  1.547813  3.692727  89.333710      1.0  2592.199951   \n994  1.004674  0.981576  0.999219  1.673958  88.629547      0.0  1875.199951   \n995  1.015231  1.051578  1.006835  1.009104  88.652969      1.0  1196.599976   \n996  1.507865  1.666354  1.546979  4.004276  89.751656      0.0  2633.250000   \n997  1.034351  1.946303  1.146284  2.952888  89.476036      1.0  2597.800049   \n\n     stat_91       stat_92  stat_93  stat_94  stat_95  \n0     4187.0  8.639500e+13      7.0      2.0     57.0  \n1     4146.0  8.639500e+13      7.0      2.0    243.0  \n2     4183.0  8.636500e+13      7.0      3.0    134.0  \n3     6000.0  8.639500e+13      7.0      4.0     72.0  \n4     4199.0  8.601500e+13      7.0      4.0     76.0  \n..       ...           ...      ...      ...      ...  \n993   4178.0  8.639500e+13      7.0      1.0     79.0  \n994   4183.0  8.639500e+13      7.0      1.0    155.0  \n995   4176.0  8.639500e+13      7.0      4.0     20.0  \n996   4188.5  8.611000e+13      7.0      3.0     85.0  \n997   4175.0  8.639500e+13      7.0      3.0     91.0  \n\n[998 rows x 96 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stat_0</th>\n      <th>stat_1</th>\n      <th>stat_2</th>\n      <th>stat_3</th>\n      <th>stat_4</th>\n      <th>stat_5</th>\n      <th>stat_6</th>\n      <th>stat_7</th>\n      <th>stat_8</th>\n      <th>stat_9</th>\n      <th>stat_10</th>\n      <th>stat_11</th>\n      <th>stat_12</th>\n      <th>stat_13</th>\n      <th>stat_14</th>\n      <th>stat_15</th>\n      <th>stat_16</th>\n      <th>stat_17</th>\n      <th>stat_18</th>\n      <th>stat_19</th>\n      <th>stat_20</th>\n      <th>stat_21</th>\n      <th>stat_22</th>\n      <th>stat_23</th>\n      <th>stat_24</th>\n      <th>stat_25</th>\n      <th>stat_26</th>\n      <th>stat_27</th>\n      <th>stat_28</th>\n      <th>stat_29</th>\n      <th>stat_30</th>\n      <th>stat_31</th>\n      <th>stat_32</th>\n      <th>stat_33</th>\n      <th>stat_34</th>\n      <th>stat_35</th>\n      <th>stat_36</th>\n      <th>stat_37</th>\n      <th>stat_38</th>\n      <th>stat_39</th>\n      <th>stat_40</th>\n      <th>stat_41</th>\n      <th>stat_42</th>\n      <th>stat_43</th>\n      <th>stat_44</th>\n      <th>stat_45</th>\n      <th>stat_46</th>\n      <th>stat_47</th>\n      <th>stat_48</th>\n      <th>stat_49</th>\n      <th>stat_50</th>\n      <th>stat_51</th>\n      <th>stat_52</th>\n      <th>stat_53</th>\n      <th>stat_54</th>\n      <th>stat_55</th>\n      <th>stat_56</th>\n      <th>stat_57</th>\n      <th>stat_58</th>\n      <th>stat_59</th>\n      <th>stat_60</th>\n      <th>stat_61</th>\n      <th>stat_62</th>\n      <th>stat_63</th>\n      <th>stat_64</th>\n      <th>stat_65</th>\n      <th>stat_66</th>\n      <th>stat_67</th>\n      <th>stat_68</th>\n      <th>stat_69</th>\n      <th>stat_70</th>\n      <th>stat_71</th>\n      <th>stat_72</th>\n      <th>stat_73</th>\n      <th>stat_74</th>\n      <th>stat_75</th>\n      <th>stat_76</th>\n      <th>stat_77</th>\n      <th>stat_78</th>\n      <th>stat_79</th>\n      <th>stat_80</th>\n      <th>stat_81</th>\n      <th>stat_82</th>\n      <th>stat_83</th>\n      <th>stat_84</th>\n      <th>stat_85</th>\n      <th>stat_86</th>\n      <th>stat_87</th>\n      <th>stat_88</th>\n      <th>stat_89</th>\n      <th>stat_90</th>\n      <th>stat_91</th>\n      <th>stat_92</th>\n      <th>stat_93</th>\n      <th>stat_94</th>\n      <th>stat_95</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>50458.0</td>\n      <td>-0.054638</td>\n      <td>-0.163923</td>\n      <td>-0.114302</td>\n      <td>0.045252</td>\n      <td>-7.805897</td>\n      <td>0.000000</td>\n      <td>46.009533</td>\n      <td>4027.514893</td>\n      <td>5.415475e+13</td>\n      <td>4.438860</td>\n      <td>2.000000</td>\n      <td>30.202068</td>\n      <td>0.633126</td>\n      <td>0.513286</td>\n      <td>0.500372</td>\n      <td>0.132576</td>\n      <td>34.917873</td>\n      <td>0.000000</td>\n      <td>205.862213</td>\n      <td>108.451317</td>\n      <td>1.876976e+13</td>\n      <td>1.825557</td>\n      <td>0.000000</td>\n      <td>11.773107</td>\n      <td>-1.812031</td>\n      <td>-2.631380</td>\n      <td>-1.798073</td>\n      <td>0.0</td>\n      <td>-89.987045</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3829.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>15.0</td>\n      <td>-0.701660</td>\n      <td>-0.619076</td>\n      <td>-0.536432</td>\n      <td>0.007953</td>\n      <td>-32.948602</td>\n      <td>0.000000</td>\n      <td>2.520257</td>\n      <td>3958.000000</td>\n      <td>4.325125e+13</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>17.0</td>\n      <td>0.015846</td>\n      <td>-0.141810</td>\n      <td>-0.104193</td>\n      <td>0.019257</td>\n      <td>-6.358004</td>\n      <td>0.0</td>\n      <td>8.230733</td>\n      <td>4029.0</td>\n      <td>5.630500e+13</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>28.0</td>\n      <td>0.437897</td>\n      <td>0.148919</td>\n      <td>0.223770</td>\n      <td>0.036048</td>\n      <td>13.095750</td>\n      <td>0.0</td>\n      <td>24.750000</td>\n      <td>4146.000000</td>\n      <td>6.978000e+13</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>38.0</td>\n      <td>1.850391</td>\n      <td>3.580182</td>\n      <td>1.738203</td>\n      <td>5.314874</td>\n      <td>89.422226</td>\n      <td>0.0</td>\n      <td>2626.199951</td>\n      <td>4187.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>57.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>340584.0</td>\n      <td>0.113277</td>\n      <td>0.093139</td>\n      <td>-0.106038</td>\n      <td>0.028960</td>\n      <td>-6.065619</td>\n      <td>0.046508</td>\n      <td>56.437958</td>\n      <td>3829.466064</td>\n      <td>4.331149e+13</td>\n      <td>3.840885</td>\n      <td>2.000000</td>\n      <td>232.909103</td>\n      <td>0.507897</td>\n      <td>0.541129</td>\n      <td>0.603787</td>\n      <td>0.096825</td>\n      <td>44.034721</td>\n      <td>0.208482</td>\n      <td>206.625092</td>\n      <td>167.600983</td>\n      <td>2.509136e+13</td>\n      <td>1.957999</td>\n      <td>0.000000</td>\n      <td>5.701968</td>\n      <td>-1.807955</td>\n      <td>-2.887664</td>\n      <td>-1.004992</td>\n      <td>0.0</td>\n      <td>-89.654587</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>223.0</td>\n      <td>-0.231743</td>\n      <td>-0.257600</td>\n      <td>-0.595426</td>\n      <td>0.000367</td>\n      <td>-37.326844</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>3724.000000</td>\n      <td>2.128500e+13</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>228.0</td>\n      <td>0.094074</td>\n      <td>0.068143</td>\n      <td>-0.228500</td>\n      <td>0.005257</td>\n      <td>-13.454103</td>\n      <td>0.0</td>\n      <td>10.050480</td>\n      <td>3812.0</td>\n      <td>4.360500e+13</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>233.0</td>\n      <td>0.517859</td>\n      <td>0.542323</td>\n      <td>0.312333</td>\n      <td>0.020598</td>\n      <td>18.462269</td>\n      <td>0.0</td>\n      <td>27.490936</td>\n      <td>3958.000000</td>\n      <td>6.511000e+13</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>238.0</td>\n      <td>1.928769</td>\n      <td>3.234613</td>\n      <td>2.475326</td>\n      <td>3.966906</td>\n      <td>89.080330</td>\n      <td>1.0</td>\n      <td>2628.199951</td>\n      <td>4146.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>243.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>40003.0</td>\n      <td>-0.499738</td>\n      <td>0.046381</td>\n      <td>-0.181152</td>\n      <td>0.056544</td>\n      <td>-11.934993</td>\n      <td>0.000000</td>\n      <td>77.305130</td>\n      <td>4106.425781</td>\n      <td>4.481677e+13</td>\n      <td>3.148264</td>\n      <td>3.000000</td>\n      <td>100.144516</td>\n      <td>0.454021</td>\n      <td>0.510668</td>\n      <td>0.412588</td>\n      <td>0.140594</td>\n      <td>27.367514</td>\n      <td>0.000000</td>\n      <td>274.848145</td>\n      <td>50.734318</td>\n      <td>2.038156e+13</td>\n      <td>1.169176</td>\n      <td>0.000000</td>\n      <td>5.653936</td>\n      <td>-1.903281</td>\n      <td>-3.150104</td>\n      <td>-1.020313</td>\n      <td>0.0</td>\n      <td>-89.540176</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3853.000000</td>\n      <td>4.500000e+10</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>97.0</td>\n      <td>-0.873151</td>\n      <td>-0.255299</td>\n      <td>-0.485521</td>\n      <td>0.005643</td>\n      <td>-30.154542</td>\n      <td>0.000000</td>\n      <td>2.918126</td>\n      <td>4089.625000</td>\n      <td>2.888500e+13</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>98.0</td>\n      <td>-0.644505</td>\n      <td>0.088542</td>\n      <td>-0.191693</td>\n      <td>0.018467</td>\n      <td>-11.570901</td>\n      <td>0.0</td>\n      <td>7.863636</td>\n      <td>4111.0</td>\n      <td>4.727000e+13</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>99.0</td>\n      <td>-0.242422</td>\n      <td>0.381953</td>\n      <td>0.088555</td>\n      <td>0.048282</td>\n      <td>5.009753</td>\n      <td>0.0</td>\n      <td>21.022933</td>\n      <td>4140.000000</td>\n      <td>6.094500e+13</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>100.0</td>\n      <td>1.021510</td>\n      <td>1.016589</td>\n      <td>1.746797</td>\n      <td>5.066334</td>\n      <td>86.987267</td>\n      <td>0.0</td>\n      <td>2618.199951</td>\n      <td>4183.0</td>\n      <td>8.636500e+13</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>134.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>223915.0</td>\n      <td>0.007430</td>\n      <td>0.007583</td>\n      <td>-0.196510</td>\n      <td>0.053544</td>\n      <td>-12.847143</td>\n      <td>0.000000</td>\n      <td>9.369678</td>\n      <td>3958.604492</td>\n      <td>4.836642e+13</td>\n      <td>4.273992</td>\n      <td>2.303057</td>\n      <td>60.025017</td>\n      <td>0.586100</td>\n      <td>0.542189</td>\n      <td>0.474437</td>\n      <td>0.103401</td>\n      <td>32.552841</td>\n      <td>0.000000</td>\n      <td>54.104408</td>\n      <td>122.706802</td>\n      <td>1.868773e+13</td>\n      <td>2.023705</td>\n      <td>1.487018</td>\n      <td>7.396456</td>\n      <td>-1.684624</td>\n      <td>-2.405738</td>\n      <td>-1.023798</td>\n      <td>0.0</td>\n      <td>-89.968369</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3468.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>48.0</td>\n      <td>-0.530198</td>\n      <td>-0.412805</td>\n      <td>-0.556091</td>\n      <td>0.009947</td>\n      <td>-34.965618</td>\n      <td>0.000000</td>\n      <td>0.893617</td>\n      <td>3841.000000</td>\n      <td>3.526000e+13</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>53.0</td>\n      <td>0.022344</td>\n      <td>0.009674</td>\n      <td>-0.245181</td>\n      <td>0.027653</td>\n      <td>-15.000056</td>\n      <td>0.0</td>\n      <td>2.340206</td>\n      <td>3947.0</td>\n      <td>4.881000e+13</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>60.0</td>\n      <td>0.536801</td>\n      <td>0.443383</td>\n      <td>0.084469</td>\n      <td>0.057278</td>\n      <td>4.816339</td>\n      <td>0.0</td>\n      <td>6.200000</td>\n      <td>4064.000000</td>\n      <td>6.330000e+13</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>67.0</td>\n      <td>5.908000</td>\n      <td>2.083693</td>\n      <td>1.269051</td>\n      <td>6.134459</td>\n      <td>89.976074</td>\n      <td>0.0</td>\n      <td>2502.000000</td>\n      <td>6000.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>72.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>15420.0</td>\n      <td>0.086653</td>\n      <td>-0.115162</td>\n      <td>-0.138969</td>\n      <td>0.040399</td>\n      <td>-11.009835</td>\n      <td>0.000000</td>\n      <td>5.049157</td>\n      <td>3992.347656</td>\n      <td>5.833895e+13</td>\n      <td>4.541829</td>\n      <td>4.000000</td>\n      <td>46.192024</td>\n      <td>0.509845</td>\n      <td>0.494897</td>\n      <td>0.639449</td>\n      <td>0.090201</td>\n      <td>47.933723</td>\n      <td>0.000000</td>\n      <td>15.590773</td>\n      <td>126.121590</td>\n      <td>2.146206e+13</td>\n      <td>2.081796</td>\n      <td>0.000000</td>\n      <td>18.615358</td>\n      <td>-1.675859</td>\n      <td>-1.071042</td>\n      <td>-1.012266</td>\n      <td>0.0</td>\n      <td>-89.770241</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3815.083252</td>\n      <td>3.500000e+10</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>20.0</td>\n      <td>-0.224805</td>\n      <td>-0.444297</td>\n      <td>-0.685736</td>\n      <td>0.005364</td>\n      <td>-46.348264</td>\n      <td>0.000000</td>\n      <td>1.438378</td>\n      <td>3837.333252</td>\n      <td>5.161375e+13</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>32.0</td>\n      <td>0.053034</td>\n      <td>-0.087422</td>\n      <td>-0.225430</td>\n      <td>0.024135</td>\n      <td>-13.665493</td>\n      <td>0.0</td>\n      <td>2.897436</td>\n      <td>4000.0</td>\n      <td>6.427000e+13</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>42.0</td>\n      <td>0.544297</td>\n      <td>0.153125</td>\n      <td>0.347474</td>\n      <td>0.043690</td>\n      <td>20.726226</td>\n      <td>0.0</td>\n      <td>4.942201</td>\n      <td>4087.000000</td>\n      <td>7.393625e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>69.0</td>\n      <td>3.231563</td>\n      <td>1.033620</td>\n      <td>1.071875</td>\n      <td>2.774382</td>\n      <td>89.300034</td>\n      <td>0.0</td>\n      <td>1046.800049</td>\n      <td>4199.0</td>\n      <td>8.601500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>76.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>393240.0</td>\n      <td>-0.147508</td>\n      <td>-0.047232</td>\n      <td>-0.242875</td>\n      <td>0.027135</td>\n      <td>-18.903458</td>\n      <td>0.222337</td>\n      <td>10.387013</td>\n      <td>3841.772705</td>\n      <td>4.316802e+13</td>\n      <td>4.002807</td>\n      <td>1.000000</td>\n      <td>67.532288</td>\n      <td>0.478085</td>\n      <td>0.499994</td>\n      <td>0.622155</td>\n      <td>0.109624</td>\n      <td>48.017563</td>\n      <td>0.410910</td>\n      <td>75.709877</td>\n      <td>164.142853</td>\n      <td>2.506494e+13</td>\n      <td>1.929882</td>\n      <td>0.000000</td>\n      <td>6.580971</td>\n      <td>-1.508058</td>\n      <td>-2.958281</td>\n      <td>-1.013423</td>\n      <td>0.0</td>\n      <td>-89.887924</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>56.0</td>\n      <td>-0.552659</td>\n      <td>-0.354082</td>\n      <td>-0.850300</td>\n      <td>0.000000</td>\n      <td>-58.557291</td>\n      <td>0.000000</td>\n      <td>0.555556</td>\n      <td>3741.000000</td>\n      <td>2.137000e+13</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>-0.112749</td>\n      <td>0.003331</td>\n      <td>-0.333463</td>\n      <td>0.002575</td>\n      <td>-20.125556</td>\n      <td>0.0</td>\n      <td>2.107143</td>\n      <td>3812.0</td>\n      <td>4.307000e+13</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>68.0</td>\n      <td>0.140716</td>\n      <td>0.280936</td>\n      <td>0.231454</td>\n      <td>0.012770</td>\n      <td>13.528161</td>\n      <td>0.0</td>\n      <td>5.281850</td>\n      <td>3958.000000</td>\n      <td>6.502500e+13</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>73.0</td>\n      <td>0.999923</td>\n      <td>1.043029</td>\n      <td>1.547813</td>\n      <td>3.692727</td>\n      <td>89.333710</td>\n      <td>1.0</td>\n      <td>2592.199951</td>\n      <td>4178.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>79.0</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>40085.0</td>\n      <td>-0.441574</td>\n      <td>-0.080691</td>\n      <td>-0.270330</td>\n      <td>0.037183</td>\n      <td>-17.535593</td>\n      <td>0.000000</td>\n      <td>11.325677</td>\n      <td>4123.798828</td>\n      <td>4.597792e+13</td>\n      <td>2.487963</td>\n      <td>1.000000</td>\n      <td>154.201294</td>\n      <td>0.502446</td>\n      <td>0.457471</td>\n      <td>0.470241</td>\n      <td>0.064660</td>\n      <td>32.590225</td>\n      <td>0.000000</td>\n      <td>35.017689</td>\n      <td>28.219002</td>\n      <td>2.436134e+13</td>\n      <td>2.188225</td>\n      <td>0.000000</td>\n      <td>0.726917</td>\n      <td>-1.073320</td>\n      <td>-1.455156</td>\n      <td>-1.016536</td>\n      <td>0.0</td>\n      <td>-87.998444</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4073.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>153.0</td>\n      <td>-0.831641</td>\n      <td>-0.369779</td>\n      <td>-0.664401</td>\n      <td>0.009702</td>\n      <td>-41.512409</td>\n      <td>0.000000</td>\n      <td>2.748235</td>\n      <td>4099.000000</td>\n      <td>2.505000e+13</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>154.0</td>\n      <td>-0.599089</td>\n      <td>-0.068216</td>\n      <td>-0.282813</td>\n      <td>0.020775</td>\n      <td>-16.773024</td>\n      <td>0.0</td>\n      <td>5.729136</td>\n      <td>4123.0</td>\n      <td>4.992000e+13</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>154.0</td>\n      <td>-0.214362</td>\n      <td>0.210247</td>\n      <td>0.034375</td>\n      <td>0.039810</td>\n      <td>1.885406</td>\n      <td>0.0</td>\n      <td>10.699164</td>\n      <td>4146.000000</td>\n      <td>6.662500e+13</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>155.0</td>\n      <td>1.004674</td>\n      <td>0.981576</td>\n      <td>0.999219</td>\n      <td>1.673958</td>\n      <td>88.629547</td>\n      <td>0.0</td>\n      <td>1875.199951</td>\n      <td>4183.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>155.0</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>342324.0</td>\n      <td>-0.181627</td>\n      <td>-0.300301</td>\n      <td>0.240738</td>\n      <td>0.002993</td>\n      <td>14.728157</td>\n      <td>0.749290</td>\n      <td>5.465665</td>\n      <td>3891.058105</td>\n      <td>4.310022e+13</td>\n      <td>4.144179</td>\n      <td>2.860763</td>\n      <td>10.118834</td>\n      <td>0.260311</td>\n      <td>0.324098</td>\n      <td>0.799344</td>\n      <td>0.009499</td>\n      <td>60.556572</td>\n      <td>0.421324</td>\n      <td>29.646894</td>\n      <td>124.940826</td>\n      <td>2.503642e+13</td>\n      <td>1.964386</td>\n      <td>1.455972</td>\n      <td>5.731455</td>\n      <td>-1.019361</td>\n      <td>-1.177506</td>\n      <td>-1.011560</td>\n      <td>0.0</td>\n      <td>-89.530304</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3718.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.267668</td>\n      <td>-0.609891</td>\n      <td>-0.947463</td>\n      <td>0.000000</td>\n      <td>-72.318169</td>\n      <td>0.488889</td>\n      <td>0.872747</td>\n      <td>3788.000000</td>\n      <td>2.139500e+13</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>-0.144425</td>\n      <td>-0.288146</td>\n      <td>0.719540</td>\n      <td>0.000011</td>\n      <td>46.380806</td>\n      <td>1.0</td>\n      <td>2.205128</td>\n      <td>3848.5</td>\n      <td>4.279000e+13</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>10.0</td>\n      <td>-0.061189</td>\n      <td>-0.132583</td>\n      <td>0.863528</td>\n      <td>0.004280</td>\n      <td>57.963976</td>\n      <td>1.0</td>\n      <td>5.808605</td>\n      <td>3982.000000</td>\n      <td>6.500000e+13</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>1.015231</td>\n      <td>1.051578</td>\n      <td>1.006835</td>\n      <td>1.009104</td>\n      <td>88.652969</td>\n      <td>1.0</td>\n      <td>1196.599976</td>\n      <td>4176.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>43330.0</td>\n      <td>-0.316384</td>\n      <td>0.016009</td>\n      <td>-0.167890</td>\n      <td>0.047388</td>\n      <td>-10.580416</td>\n      <td>0.000000</td>\n      <td>42.296310</td>\n      <td>4053.579102</td>\n      <td>5.046215e+13</td>\n      <td>4.470182</td>\n      <td>3.000000</td>\n      <td>53.201683</td>\n      <td>0.453665</td>\n      <td>0.502702</td>\n      <td>0.585710</td>\n      <td>0.106351</td>\n      <td>42.947170</td>\n      <td>0.000000</td>\n      <td>208.168976</td>\n      <td>112.404045</td>\n      <td>1.942842e+13</td>\n      <td>1.931421</td>\n      <td>0.000000</td>\n      <td>14.244914</td>\n      <td>-1.746094</td>\n      <td>-2.905339</td>\n      <td>-1.048372</td>\n      <td>0.0</td>\n      <td>-89.833092</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3824.000000</td>\n      <td>5.500000e+10</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>41.0</td>\n      <td>-0.684180</td>\n      <td>-0.309863</td>\n      <td>-0.649974</td>\n      <td>0.006432</td>\n      <td>-41.541863</td>\n      <td>0.000000</td>\n      <td>2.392969</td>\n      <td>4028.666748</td>\n      <td>3.689000e+13</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>42.0</td>\n      <td>-0.366849</td>\n      <td>0.024974</td>\n      <td>-0.245378</td>\n      <td>0.023637</td>\n      <td>-15.086617</td>\n      <td>0.0</td>\n      <td>6.926828</td>\n      <td>4070.0</td>\n      <td>5.347750e+13</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>50.0</td>\n      <td>-0.010677</td>\n      <td>0.400677</td>\n      <td>0.204727</td>\n      <td>0.041420</td>\n      <td>12.220764</td>\n      <td>0.0</td>\n      <td>15.000000</td>\n      <td>4147.000000</td>\n      <td>6.640875e+13</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>53.0</td>\n      <td>1.507865</td>\n      <td>1.666354</td>\n      <td>1.546979</td>\n      <td>4.004276</td>\n      <td>89.751656</td>\n      <td>0.0</td>\n      <td>2633.250000</td>\n      <td>4188.5</td>\n      <td>8.611000e+13</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>85.0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>396396.0</td>\n      <td>-0.004272</td>\n      <td>0.016859</td>\n      <td>-0.631731</td>\n      <td>0.011926</td>\n      <td>-55.630768</td>\n      <td>0.655708</td>\n      <td>16.771982</td>\n      <td>3838.189453</td>\n      <td>4.321212e+13</td>\n      <td>3.909848</td>\n      <td>3.000000</td>\n      <td>79.435593</td>\n      <td>0.351545</td>\n      <td>0.303812</td>\n      <td>0.623476</td>\n      <td>0.024331</td>\n      <td>50.303635</td>\n      <td>0.468723</td>\n      <td>95.327438</td>\n      <td>155.573868</td>\n      <td>2.497264e+13</td>\n      <td>1.946892</td>\n      <td>0.000000</td>\n      <td>6.634319</td>\n      <td>-1.038711</td>\n      <td>-1.522690</td>\n      <td>-1.018787</td>\n      <td>0.0</td>\n      <td>-88.761833</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3098.166748</td>\n      <td>0.000000e+00</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>68.0</td>\n      <td>-0.052803</td>\n      <td>-0.044517</td>\n      <td>-1.009344</td>\n      <td>0.008622</td>\n      <td>-88.386049</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>3747.000000</td>\n      <td>2.154000e+13</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>74.0</td>\n      <td>-0.020622</td>\n      <td>-0.028179</td>\n      <td>-1.007728</td>\n      <td>0.009831</td>\n      <td>-86.119919</td>\n      <td>1.0</td>\n      <td>0.879005</td>\n      <td>3812.0</td>\n      <td>4.331000e+13</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>79.0</td>\n      <td>-0.019081</td>\n      <td>0.020307</td>\n      <td>-0.294459</td>\n      <td>0.010668</td>\n      <td>-17.483364</td>\n      <td>1.0</td>\n      <td>6.141348</td>\n      <td>3951.187561</td>\n      <td>6.485500e+13</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>85.0</td>\n      <td>1.034351</td>\n      <td>1.946303</td>\n      <td>1.146284</td>\n      <td>2.952888</td>\n      <td>89.476036</td>\n      <td>1.0</td>\n      <td>2597.800049</td>\n      <td>4175.0</td>\n      <td>8.639500e+13</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>91.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>998 rows × 96 columns</p>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\ntotal_ts_encoded = perform_autoencoder(df_total, encoding_dim=60, epochs=100, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:20.380964Z","iopub.execute_input":"2024-12-18T17:04:20.381294Z","iopub.status.idle":"2024-12-18T17:04:29.337354Z","shell.execute_reply.started":"2024-12-18T17:04:20.381258Z","shell.execute_reply":"2024-12-18T17:04:29.336407Z"}},"outputs":[{"name":"stdout","text":"Epoch [10/100], Loss: 1.3467]\nEpoch [20/100], Loss: 1.2659]\nEpoch [30/100], Loss: 1.2529]\nEpoch [40/100], Loss: 1.2513]\nEpoch [50/100], Loss: 1.2478]\nEpoch [60/100], Loss: 1.2392]\nEpoch [70/100], Loss: 1.2367]\nEpoch [80/100], Loss: 1.2211]\nEpoch [90/100], Loss: 1.1825]\nEpoch [100/100], Loss: 1.1564]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"time_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:29.338702Z","iopub.execute_input":"2024-12-18T17:04:29.338996Z","iopub.status.idle":"2024-12-18T17:04:29.343081Z","shell.execute_reply.started":"2024-12-18T17:04:29.338965Z","shell.execute_reply":"2024-12-18T17:04:29.342076Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"train_sleep = pd.read_csv(\"/kaggle/input/sleep-detection/sleep_features.csv\")\ntest_sleep = pd.read_csv(\"/kaggle/working/features/test/sleep_features.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:29.344152Z","iopub.execute_input":"2024-12-18T17:04:29.344464Z","iopub.status.idle":"2024-12-18T17:04:29.367011Z","shell.execute_reply.started":"2024-12-18T17:04:29.344432Z","shell.execute_reply":"2024-12-18T17:04:29.366369Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"sleep_cols = train_sleep.columns.tolist()\nsleep_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:29.367926Z","iopub.execute_input":"2024-12-18T17:04:29.368201Z","iopub.status.idle":"2024-12-18T17:04:29.372089Z","shell.execute_reply.started":"2024-12-18T17:04:29.368176Z","shell.execute_reply":"2024-12-18T17:04:29.371254Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"rm -rf /kaggle/working/features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:29.373024Z","iopub.execute_input":"2024-12-18T17:04:29.373255Z","iopub.status.idle":"2024-12-18T17:04:30.427217Z","shell.execute_reply.started":"2024-12-18T17:04:29.373231Z","shell.execute_reply":"2024-12-18T17:04:30.426159Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"rm -rf /kaggle/working/heuristic_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:30.428947Z","iopub.execute_input":"2024-12-18T17:04:30.429941Z","iopub.status.idle":"2024-12-18T17:04:31.458735Z","shell.execute_reply.started":"2024-12-18T17:04:30.429886Z","shell.execute_reply":"2024-12-18T17:04:31.457708Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:04:31.460233Z","iopub.execute_input":"2024-12-18T17:04:31.460545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetRegressor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub2 = pd.merge(train, train_ts, how=\"left\", on='id')\ntest_sub2 = pd.merge(test, test_ts, how=\"left\", on='id')\n\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train_sub2[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\nfor col in train_sub2.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train_sub2[col]\n        \ntrain_sub2 = train_imputed\n\ntrain_sub2 = feature_engineering(train_sub2)\ntrain_sub2 = train_sub2.dropna(subset='sii', ignore_index=True)\ntest_sub2 = feature_engineering(test_sub2)\n\ntrain_sub2 = train_sub2.drop('id', axis=1)\ntest_sub2  = test_sub2.drop('id', axis=1)   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_sub2 = noseason_features + time_series_cols\n\n# train_sub2 = pd.merge(train, train_ts, how=\"left\", on='id')\n# test_sub2 = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain_sub2 = train_sub2.dropna(subset='sii')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if np.any(np.isinf(train_sub2)):\n    train_sub2 = train_sub2.replace([np.inf, -np.inf], np.nan)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_sub2 = train_sub2[features_sub2]\ny_sub2 = train_sub2['sii']\ntest_sub2 = test_sub2[features_sub2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01,  # Increased from 2.68e-06\n    'device': 'cpu'\n\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED,\n    'tree_method': 'gpu_hist',\n\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10,  # Increase this value\n    'task_type': 'GPU'\n\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# New: TabNet\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.callbacks import Callback\nimport os\nimport torch\nfrom pytorch_tabnet.callbacks import Callback\n\nclass TabNetWrapper(BaseEstimator, RegressorMixin):\n    def __init__(self, **kwargs):\n        self.model = TabNetRegressor(**kwargs)\n        self.kwargs = kwargs\n        self.imputer = SimpleImputer(strategy='median')\n        self.best_model_path = 'best_tabnet_model.pt'\n        \n    def fit(self, X, y):\n        # Handle missing values\n        X_imputed = self.imputer.fit_transform(X)\n        \n        if hasattr(y, 'values'):\n            y = y.values\n            \n        # Create internal validation set\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_imputed, \n            y, \n            test_size=0.2,\n            random_state=42\n        )\n        \n        # Train TabNet model\n        history = self.model.fit(\n            X_train=X_train,\n            y_train=y_train.reshape(-1, 1),\n            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n            eval_name=['valid'],\n            eval_metric=['mse'],\n            max_epochs=200,\n            patience=20,\n            batch_size=1024,\n            virtual_batch_size=128,\n            num_workers=0,\n            drop_last=False,\n            callbacks=[\n                TabNetPretrainedModelCheckpoint(\n                    filepath=self.best_model_path,\n                    monitor='valid_mse',\n                    mode='min',\n                    save_best_only=True,\n                    verbose=True\n                )\n            ]\n        )\n        \n        # Load the best model\n        if os.path.exists(self.best_model_path):\n            self.model.load_model(self.best_model_path)\n            os.remove(self.best_model_path)  # Remove temporary file\n        \n        return self\n    \n    def predict(self, X):\n        X_imputed = self.imputer.transform(X)\n        return self.model.predict(X_imputed).flatten()\n    \n    def __deepcopy__(self, memo):\n        # Add deepcopy support for scikit-learn\n        cls = self.__class__\n        result = cls.__new__(cls)\n        memo[id(self)] = result\n        for k, v in self.__dict__.items():\n            setattr(result, k, deepcopy(v, memo))\n        return result\n\n# TabNet hyperparameters\nTabNet_Params = {\n    'n_d': 64,              # Width of the decision prediction layer\n    'n_a': 64,              # Width of the attention embedding for each step\n    'n_steps': 5,           # Number of steps in the architecture\n    'gamma': 1.5,           # Coefficient for feature selection regularization\n    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n    'lambda_sparse': 1e-4,  # Sparsity regularization\n    'optimizer_fn': torch.optim.Adam,\n    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n    'mask_type': 'entmax',\n    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n    'verbose': 1,\n    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n}\n\nclass TabNetPretrainedModelCheckpoint(Callback):\n    def __init__(self, filepath, monitor='val_loss', mode='min', \n                 save_best_only=True, verbose=1):\n        super().__init__()  # Initialize parent class\n        self.filepath = filepath\n        self.monitor = monitor\n        self.mode = mode\n        self.save_best_only = save_best_only\n        self.verbose = verbose\n        self.best = float('inf') if mode == 'min' else -float('inf')\n        \n    def on_train_begin(self, logs=None):\n        self.model = self.trainer  # Use trainer itself as model\n        \n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        current = logs.get(self.monitor)\n        if current is None:\n            return\n        \n        # Check if current metric is better than best\n        if (self.mode == 'min' and current < self.best) or \\\n           (self.mode == 'max' and current > self.best):\n            if self.verbose:\n                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n            self.best = current\n            if self.save_best_only:\n                self.model.save_model(self.filepath)  # Save the entire model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\nTabNet_Model = TabNetWrapper(**TabNet_Params) \nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model),\n    ('tabnet', TabNet_Model)\n],weights=[4.0,4.0,5.0,4.0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission2, _, _, _, _= TrainML(voting_model, X_sub2, y_sub2, test_sub2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub5 = pd.merge(train, train_sleep, how=\"left\", on='id')\ntest_sub5 = pd.merge(test, test_sleep, how=\"left\", on='id')\n# print(train_sub5)\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train_sub5[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\nfor col in train_sub5.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train_sub5[col]\n        \ntrain_sub2 = train_imputed\n\ntrain_sub5 = feature_engineering(train_sub5)\ntrain_sub5 = train_sub5.dropna(subset='sii', ignore_index=True)\ntest_sub5 = feature_engineering(test_sub5)\n\ntrain_sub5 = train_sub5.drop('id', axis=1)\ntest_sub5  = test_sub5.drop('id', axis=1) \n\nfeatures_sub5 = noseason_features + sleep_cols\n\n# train_sub2 = pd.merge(train, train_ts, how=\"left\", on='id')\n# test_sub2 = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain_sub5 = train_sub5.dropna(subset='sii')\n\nif np.any(np.isinf(train_sub5)):\n    train_sub5 = train_sub5.replace([np.inf, -np.inf], np.nan)\n\nX_sub5 = train_sub5[features_sub5]\ny_sub5 = train_sub5['sii']\ntest_sub5 = test_sub5[features_sub5]\n\nsubmission5, _, _, _, _= TrainML(voting_model, X_sub5, y_sub5, test_sub5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub3 = pd.merge(train, train_ts, how=\"left\", on='id')\ntest_sub3 = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain_sub3 = train_sub3.drop('id', axis=1)\ntest_sub3 = test_sub3.drop('id', axis=1) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_sub3 = total_features + time_series_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub3 = train_sub3.dropna(subset='sii')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub3 = update(train_sub3)\ntest_sub3 = update(test_sub3)\n\nfor col in cat_c:\n    mapping = create_mapping(col, train_sub3)\n    mappingTe = create_mapping(col, test_sub3)\n    \n    train_sub3[col] = train_sub3[col].replace(mapping).astype(int)\n    test_sub3[col] = test_sub3[col].replace(mappingTe).astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Params = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_sub3 = train_sub3[features_sub3]\ny_sub3 = train_sub3['sii']\ntest_sub3 = test_sub3[features_sub3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission3, _, _, _, _= TrainML(voting_model, X_sub3, y_sub3, test_sub3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub6 = pd.merge(train, train_sleep, how=\"left\", on='id')\ntest_sub6 = pd.merge(test, test_sleep, how=\"left\", on='id')\n\ntrain_sub6 = train_sub6.drop('id', axis=1)\ntest_sub6 = test_sub6.drop('id', axis=1) \n\nfeatures_sub6 = total_features + sleep_cols\n\ntrain_sub6 = train_sub6.dropna(subset='sii')\n\ntrain_sub6 = update(train_sub6)\ntest_sub6 = update(test_sub6)\n\nfor col in cat_c:\n    mapping = create_mapping(col, train_sub6)\n    mappingTe = create_mapping(col, test_sub6)\n    \n    train_sub6[col] = train_sub6[col].replace(mapping).astype(int)\n    test_sub6[col] = test_sub6[col].replace(mappingTe).astype(int)\n\nX_sub6 = train_sub6[features_sub6]\ny_sub6 = train_sub6['sii']\ntest_sub6 = test_sub6[features_sub6]\n\nsubmission6, _, _, _, _= TrainML(voting_model, X_sub6, y_sub6, test_sub6)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='median')\n\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n])\n\n\nsubmission4, _, _, _, _= TrainML(ensemble, X_sub3, y_sub3, test_sub3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission7, _, _, _, _= TrainML(ensemble, X_sub6, y_sub6, test_sub6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub1 = submission2\nsub2 = submission3\nsub3 = submission4\nsub5 = submission5\nsub6 = submission6\nsub7 = submission7\n\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\nsub5 = sub5.sort_values(by='id').reset_index(drop=True)\nsub6 = sub6.sort_values(by='id').reset_index(drop=True)\nsub7 = sub7.sort_values(by='id').reset_index(drop=True)\n\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii'],\n    'sii_5': sub5['sii'],\n    'sii_6': sub6['sii'],\n    'sii_7': sub7['sii']\n})\n\ndef majority_vote(row):\n    return row.mode()[0]\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_5', 'sii_6', 'sii_7']].apply(majority_vote, axis=1)\nprint(combined)\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'Final_Submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sub1 = submission2\n# sub2 = submission3\n# sub3 = submission4\n# sub5 = submission5\n# sub6 = submission6\n# sub7 = submission7\n\n# sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n# sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n# sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n# sub5 = sub5.sort_values(by='id').reset_index(drop=True)\n# sub6 = sub6.sort_values(by='id').reset_index(drop=True)\n# sub7 = sub7.sort_values(by='id').reset_index(drop=True)\n\n# combined = pd.DataFrame({\n#     'id': sub1['id'],\n#     'sii_1': sub1['sii'],\n#     'sii_2': sub2['sii'],\n#     'sii_3': sub3['sii'],\n#     'sii_5': sub5['sii'],\n#     'sii_6': sub6['sii'],\n#     'sii_7': sub7['sii']\n# })\n\n# def majority_vote(row):\n#     return row.mode()[0]\n\n# combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_5', 'sii_6', 'sii_7']].apply(majority_vote, axis=1)\n# print(combined)\n# final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\n# final_submission.to_csv('submission.csv', index=False)\n\n# print(\"Majority voting completed and saved to 'Final_Submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}